{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c86bc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Model file not found!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m TEST_PATH = \u001b[33m\"\u001b[39m\u001b[33mdata/features/graph_features_test.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Ensure model exists\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m os.path.exists(MODEL_PATH), \u001b[33m\"\u001b[39m\u001b[33mModel file not found!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     25\u001b[39m rf, xgb, meta, scaler, best_thresh = joblib.load(MODEL_PATH)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Model loaded from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: Model file not found!"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# üìä evaluation.ipynb ‚Äî visualize performance of hybrid fake news model\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, roc_curve, auc,\n",
    "    precision_recall_curve, classification_report,\n",
    "    accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 1Ô∏è‚É£ Load model + data\n",
    "# ===============================\n",
    "MODEL_PATH = \"data/model/hybrid_model.pkl\"\n",
    "TRAIN_PATH = \"data/features/graph_features_train.csv\"\n",
    "TEST_PATH = \"data/features/graph_features_test.csv\"\n",
    "\n",
    "# Ensure model exists\n",
    "assert os.path.exists(MODEL_PATH), \"Model file not found!\"\n",
    "rf, xgb, meta, scaler, best_thresh = joblib.load(MODEL_PATH)\n",
    "\n",
    "print(f\"‚úÖ Model loaded from {MODEL_PATH}\")\n",
    "print(f\"Using threshold: {best_thresh:.3f}\")\n",
    "\n",
    "# Load merged data\n",
    "text_emb = pd.read_csv(\"data/model/text_embeddings.csv\")\n",
    "graph_train = pd.read_csv(TRAIN_PATH)\n",
    "graph_test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(f\"Text embeddings: {text_emb.shape}\")\n",
    "print(f\"Graph train: {graph_train.shape}, test: {graph_test.shape}\")\n",
    "\n",
    "# ===============================\n",
    "# 2Ô∏è‚É£ Merge text + graph features\n",
    "# ===============================\n",
    "merged_test = pd.merge(\n",
    "    text_emb, graph_test, left_on=\"id\", right_on=\"node_id\", how=\"inner\"\n",
    ")\n",
    "merged_test = merged_test.dropna(subset=[\"label\"]).reset_index(drop=True)\n",
    "\n",
    "# Identify feature columns\n",
    "non_feature_cols = [\"id\", \"source\", \"node_id\", \"label\"]\n",
    "X_test = merged_test.drop(columns=[c for c in non_feature_cols if c in merged_test.columns])\n",
    "y_test = merged_test[\"label\"]\n",
    "\n",
    "X_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ===============================\n",
    "# 3Ô∏è‚É£ Make predictions\n",
    "# ===============================\n",
    "rf_prob = rf.predict_proba(X_scaled)[:, 1]\n",
    "xgb_prob = xgb.predict_proba(X_scaled)[:, 1]\n",
    "meta_input = np.vstack([rf_prob, xgb_prob]).T\n",
    "meta_prob = meta.predict_proba(meta_input)[:, 1]\n",
    "y_pred = (meta_prob > best_thresh).astype(int)\n",
    "\n",
    "# ===============================\n",
    "# 4Ô∏è‚É£ Compute Metrics\n",
    "# ===============================\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, meta_prob)\n",
    "precisions, recalls, _ = precision_recall_curve(y_test, meta_prob)\n",
    "pr_auc = auc(recalls, precisions)\n",
    "\n",
    "print(\"\\n=== üìä Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"‚úÖ Accuracy: {acc:.3f}\")\n",
    "print(f\"‚úÖ F1-score: {f1:.3f}\")\n",
    "print(f\"‚úÖ ROC-AUC: {roc_auc:.3f}\")\n",
    "print(f\"‚úÖ PR-AUC: {pr_auc:.3f}\")\n",
    "\n",
    "# ===============================\n",
    "# 5Ô∏è‚É£ Confusion Matrix\n",
    "# ===============================\n",
    "plt.figure(figsize=(5,4))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# 6Ô∏è‚É£ ROC Curve\n",
    "# ===============================\n",
    "fpr, tpr, _ = roc_curve(y_test, meta_prob)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# 7Ô∏è‚É£ Precision-Recall Curve\n",
    "# ===============================\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(recalls, precisions, label=f\"PR-AUC = {pr_auc:.3f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# 8Ô∏è‚É£ Feature Importance (RF)\n",
    "# ===============================\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[-20:]\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh(range(len(indices)), importances[indices], align=\"center\")\n",
    "plt.yticks(range(len(indices)), np.array(X_test.columns)[indices])\n",
    "plt.title(\"Top 20 RandomForest Feature Importances\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# 9Ô∏è‚É£ Summary\n",
    "# ===============================\n",
    "print(\"\\n=== üß© Model Summary ===\")\n",
    "print(f\"Samples tested: {len(y_test)}\")\n",
    "print(f\"Fake label threshold: {best_thresh:.3f}\")\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(f\"F1 (Fake): {f1:.3f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
    "print(f\"PR-AUC: {pr_auc:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
